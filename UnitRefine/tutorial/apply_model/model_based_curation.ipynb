{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IupkZSc2bfE"
   },
   "source": [
    "# Model-Based Curation Tutorial: Applying Pre-trained Models on Sorting Analyzer\n",
    "\n",
    "## Overview\n",
    "\n",
    "This comprehensive tutorial demonstrates how to use machine learning classifiers for automated curation of spike-sorted neural data. You'll learn to download and apply pre-trained models from Hugging Face to classify unit quality using SpikeInterface's powerful analysis framework.\n",
    "\n",
    "\n",
    "## Tutorial Structure\n",
    "\n",
    "This tutorial follows a systematic workflow:\n",
    "1. **Setup and Imports**: Configure environment and load necessary packages\n",
    "2. **Model Loading**: Download and examine pre-trained classification models  \n",
    "3. **Data Preparation**: Generate synthetic data with required quality metrics\n",
    "4. **Model Application**: Apply models to classify unit quality automatically\n",
    "5. **Performance Evaluation**: Compare predictions with human labels\n",
    "6. **Confidence Analysis**: Analyze prediction reliability for workflow optimization\n",
    "\n",
    "Let's begin by setting up our environment and importing the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Package Imports\n",
    "\n",
    "First, we'll import all necessary packages and configure our environment for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQ2-74dw2bfF"
   },
   "outputs": [],
   "source": [
    "# Essential imports for data handling and visualization\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# SpikeInterface imports for neurophysiology data analysis\n",
    "import spikeinterface as si\n",
    "import spikeinterface.widgets as sw\n",
    "\n",
    "# Machine learning imports for model evaluation\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "print(\"✓ Standard packages imported successfully\")\n",
    "print(f\"✓ SpikeInterface version: {si.__version__}\")\n",
    "\n",
    "si.set_global_job_kwargs(n_jobs=4)\n",
    "# Configure matplotlib for better plots\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnitRefine import \n",
    "try:\n",
    "    from UnitRefine.scripts.model_based_curation import load_model, auto_label_units\n",
    "    print(\"✓ UnitRefine imported successfully\")\n",
    "except ImportError:\n",
    "    # If UnitRefine is not installed, add the local path\n",
    "    import sys\n",
    "    # Navigate to the directory containing the UnitRefine folder\n",
    "    current_path = Path.cwd()\n",
    "    unitrefine_root = current_path.parent.parent.parent  # Go to project root\n",
    "    sys.path.append(str(unitrefine_root))\n",
    "    \n",
    "    try:\n",
    "        from UnitRefine.scripts.model_based_curation import load_model, auto_label_units\n",
    "        print(\"✓ UnitRefine imported from local path\")\n",
    "    except ImportError:\n",
    "        print(\"❌ Could not import UnitRefine. Please check installation or path.\")\n",
    "        print(f\"   Attempted path: {unitrefine_root}\")\n",
    "        print(\"   Alternative: Install via 'pip install UnitRefine' or check SpikeInterface integration\")\n",
    "        \n",
    "# Alternative import (if integrated with SpikeInterface)\n",
    "# from spikeinterface.curation import load_model, auto_label_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1IOog1o2bfG"
   },
   "source": [
    "## Step 2: Loading Pre-trained Models\n",
    "\n",
    "### Understanding Model Sources\n",
    "\n",
    "UnitRefine supports loading pre-trained models from multiple sources:\n",
    "- **Hugging Face Hub**: Public repository of community models\n",
    "- **Local Storage**: Previously downloaded or custom-trained models\n",
    "\n",
    "The `load_model` function handles these sources automatically and provides model metadata including feature requirements, training details, and performance metrics.\n",
    "\n",
    "### Downloading from Hugging Face\n",
    "\n",
    "Let's download a pre-trained model from [Hugging Face](https://huggingface.co/). This example uses a toy tetrode model specifically designed for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvj_eSp92bfH"
   },
   "outputs": [],
   "source": [
    "# Download and load pre-trained model from Hugging Face\n",
    "print(\"Downloading pre-trained model from Hugging Face...\")\n",
    "try:\n",
    "    model, model_info = load_model(\n",
    "        repo_id=\"SpikeInterface/toy_tetrode_model\",\n",
    "        trusted=['numpy.dtype']\n",
    "    )\n",
    "    print(\"✓ Model loaded successfully!\")\n",
    "    print(f\"✓ Model type: {type(model).__name__}\")\n",
    "    print(f\"✓ Number of features required: {len(model.feature_names_in_)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {str(e)}\")\n",
    "    print(\"   Check internet connection and Hugging Face access\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TS2gB-Kd2bfH"
   },
   "source": [
    "### Examining the Model\n",
    "\n",
    "The loaded model is a scikit-learn Pipeline that contains preprocessing steps and the trained classifier. Let's explore its structure and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1730500463640,
     "user": {
      "displayName": "neuro work",
      "userId": "09036868344714389966"
     },
     "user_tz": -60
    },
    "id": "ZIB5C1Hi2bfH",
    "outputId": "e36a9c47-0fbc-4e24-a00b-1ec563b6f494"
   },
   "outputs": [],
   "source": [
    "# Display model information\n",
    "print(\"Model Structure:\")\n",
    "print(f\"Pipeline steps: {[step[0] for step in model.steps]}\")\n",
    "print(f\"Final estimator: {model.steps[-1][1].__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZdqwIGy2bfH"
   },
   "source": [
    "### Understanding Feature Requirements\n",
    "\n",
    "Each model requires specific quality metrics to make predictions. Let's examine what metrics this model expects and understand their significance in spike sorting quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1730500466592,
     "user": {
      "displayName": "neuro work",
      "userId": "09036868344714389966"
     },
     "user_tz": -60
    },
    "id": "EORNzx7_2bfH",
    "outputId": "f8f2544c-88a3-4eb7-fc9e-f4bfae1d1dc7"
   },
   "outputs": [],
   "source": [
    "# Display required features (metrics)\n",
    "required_features = model.feature_names_in_\n",
    "print(\"Required Quality Metrics:\")\n",
    "\n",
    "print(f\"\\nTotal features required: {len(required_features)}\")\n",
    "\n",
    "# Display model metadata if available\n",
    "if 'training_info' in model_info:\n",
    "    print(f\"\\nModel Training Information:\")\n",
    "    training_info = model_info['training_info']\n",
    "    if 'dataset_size' in training_info:\n",
    "        print(f\"Training dataset size: {training_info['dataset_size']} units\")\n",
    "    if 'accuracy' in training_info:\n",
    "        print(f\"Training accuracy: {training_info['accuracy']:.1%}\")\n",
    "        \n",
    "print(f\"\\nLabel conversion: {model_info.get('label_conversion', 'Not specified')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqwzjpOq2bfH"
   },
   "source": [
    "## Step 3: Data Preparation and Analysis Setup\n",
    "\n",
    "### Understanding SortingAnalyzer Requirements\n",
    "\n",
    "To use this model, we need to create a `SortingAnalyzer` object with all required metrics computed. This involves:\n",
    "1. **Recording object**: loading electrophysiology data\n",
    "2. **Spike Sorting object**: Running a spike sorter and loading the output \n",
    "3. **Metric Computation**: Calculating quality and template metrics\n",
    "\n",
    "\n",
    "For this tutorial, we'll generate synthetic data that mimics real tetrode recordings.\n",
    "\n",
    "**Key Resources:**\n",
    "- [Recordings Documentation](https://spikeinterface.readthedocs.io/en/latest/modules/extractors.html)\n",
    "- [Spike Sorting Guide](https://spikeinterface.readthedocs.io/en/latest/modules/sorters.html) \n",
    "- [SortingAnalyzer Tutorial](https://spikeinterface.readthedocs.io/en/latest/tutorials/core/plot_4_sorting_analyzer.html)\n",
    "- [Extensions Overview](https://spikeinterface.readthedocs.io/en/latest/modules/postprocessing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273,
     "referenced_widgets": [
      "c3a573c01eda4291b7a48dab8398b4ed",
      "b127bfc0ec484cc89c3e29308e997288",
      "62fb9fde23924c0a9543d8d710a917b0",
      "e52b293e127b45ce815e7a7200a9da88",
      "86c382c0ee334e0c949f18b20467b592",
      "d9908add4472481fab2fa3738894981a",
      "88c7a0fbea244747bebab4cfc8277e4e",
      "5e17a2dfc5434d48a0f08d3d92c25453",
      "84d01b065a7e410eac336e3e13ca0725",
      "586418fb2fdd4401ab1e14f15c62e838",
      "372963efa72e4d1fb47d20825556be48",
      "1b649842e337481e8864c8ec3b242784",
      "f726f3fc56a44e409c33d8be6304455a",
      "001f16a432714a10af4f8974c7fc35d4",
      "66988217078f40609bd9e8fc1785c57c",
      "5ad6e50777a24d32b88e2bf8da36dd48",
      "49f15bf7faaf4f53a73d0ae3d42d41a8",
      "c4b6ad768a1f41328dd5373ad38b192e",
      "fffe4496802340379d23fc106a42e828",
      "eab4646f581e4c0992ad54d95e27dd6b",
      "0eedc2cc46674418925aeb5395fdbaf4",
      "5a152639bc0b4470a161746b759334ad",
      "004638b86d274733954a3cdeef79299a",
      "cafe7bf6be6e4516be6fade869e93727",
      "fba6c38ba09a4ed58366518862bd997a",
      "12d1b739f65f426f8be49d7942ec02a7",
      "5eab47a0ecc14a68ac2db50276d83efe",
      "24233e33c662491eafd4ce48fefae0ec",
      "b619794a22d245eb8cfa0be47845f7bd",
      "f7007979f7ac4111a833c8948e4831d0",
      "18210a69016f4c268aefc55dccdbdef8",
      "bf91a442bb604c04b9e451538de9991c",
      "fb8ae1173ce4462987b2a34b34681ec6",
      "9b09999b6b8a4871bc3db4bb2c11dd89",
      "aa9dde7fc9c847ceaf1829f364b83471",
      "fdf56538063c43f1b8c0dd05da718a8d",
      "f2bce0b2f72d4dcbb8bc44e3bff65734",
      "5d4d2bd13ebb465587c4b816afcbd154",
      "b8a88654b8534319b0b78d24fe024c48",
      "f11bd8829438414eb6b2cbd6172f1111",
      "672f3c4a7e534891884c5918a0448532",
      "beba5280410c433bafbb6dd3d4bae37d",
      "5e66a407ae4b44baaf9e922fdec10e73",
      "9244b59122c04672a17a0c09c9ac0fe6",
      "a73216b7f460403481d697695661dea7",
      "3a6cf6c75e7b42b78cb73593651e2a77",
      "476b2fd98a6e45a8a4e210ab77853e32",
      "331c44ecc9b748ce86eb41345e5c4f11",
      "35d87972e60b40e288bdfea491dbc689",
      "10d76a29c0f24d418152787485206794",
      "96010da60e5044d9a7f5a4511b20caf3",
      "27901508ea124fcfae002b5661170442",
      "ea07496e21384e2f92c529f19f840ce1",
      "bb9c021bac1a4bb8b81169587a0b33ab",
      "960fdabcc5db400fb114d438a2a6629d",
      "cfb08fd7ebae4a329b2c604a53d5d668",
      "a24be1b5dcf14151884d639ad42fcfde",
      "d9cb1e30327b4c80965cfa6b634c365e",
      "bf2ca10f634f44c4832c97a57615b0ba",
      "7bf89ea8715d4432a59a48caefc2e05f",
      "c332ee4e99664f1da4bcbb8c147ccf50",
      "4a77bccfe55a4dcf894ec8e8e2dfb359",
      "caa9d79b506f4c69abebfa0f1c58eb7c",
      "c6db8245526e4a04b15a1d3eaabb67e4",
      "0f7239d3cc4948b590fd307e9ed3e7e8",
      "2b8b82ea26034c13a8daf509bf590975",
      "fecc5fa17f17494899ce48559c6d7973",
      "4a3a204f801443d9937211b2eef50a9a",
      "d114d4de8c6d432cbaee9f6aa9abe0ac",
      "02b11e3a8949446f99ccc6abf87615cd",
      "9be2b70a121c41cfb18c7f76de6959c3",
      "914a54b488684e3286b8093739afc3de",
      "a7d1c0e9b5a740b293e325cd7eff50cb",
      "4ada8e75ce6d46dbb33d5e415f78d8d0",
      "c9c28aa2ff7f46138896b63d38df4523",
      "ed17f900f9e44377ba1936e5b9e51b92",
      "13c4a6937eac4afa822f8cc6141239e1",
      "ed80abc9bd7d4ba3887020a8d646d973",
      "5c1a2b6241634b06ad54adf210fccab5",
      "7663b8a2c3dc4bdfbe78750ace8b3e51",
      "e75c64cc494f47cab633018fba8a62c0",
      "0e2f9d45b3d74293b4c5c6670b73d031",
      "61d18888b9ad44f29e211f52429bebfd",
      "cc9d6aeef2b64ac2a569010dbf5281da",
      "2dace4d3b3194af0be8c0e68ddd771a0",
      "a9a9d47032a447af87ca7bea139fc89e",
      "790bb30932fe4dee80d4ae9370616f7f",
      "c718ca17612a4d8ab420943b401611f8"
     ]
    },
    "executionInfo": {
     "elapsed": 11970,
     "status": "ok",
     "timestamp": 1730500483087,
     "user": {
      "displayName": "neuro work",
      "userId": "09036868344714389966"
     },
     "user_tz": -60
    },
    "id": "fas3byXD2bfH",
    "outputId": "4f94a4de-3201-4c0c-b02b-ca96c120b326"
   },
   "outputs": [],
   "source": [
    "# Generate synthetic tetrode data matching the model's training domain\n",
    "print(\"Generating synthetic tetrode recording and sorting...\")\n",
    "\n",
    "# Create synthetic data with parameters matching model training\n",
    "recording, sorting = si.generate_ground_truth_recording(\n",
    "    num_channels=4,        # Tetrode configuration\n",
    "    seed=4,               # Reproducible results\n",
    "    num_units=10,         # Moderate number of units\n",
    "    sampling_frequency=30000,  # Standard sampling rate\n",
    "    durations=[60],       # 60 seconds of data\n",
    "    \n",
    ")\n",
    "\n",
    "print(f\"✓ Generated recording: {recording.get_num_channels()} channels, \"\n",
    "      f\"{recording.get_sampling_frequency()} Hz, {recording.get_total_duration():.1f}s\")\n",
    "print(f\"✓ Generated sorting: {sorting.get_num_units()} units\")\n",
    "\n",
    "# Create SortingAnalyzer for comprehensive analysis\n",
    "print(\"\\nCreating SortingAnalyzer and computing extensions...\")\n",
    "sorting_analyzer = si.create_sorting_analyzer(\n",
    "    sorting=sorting, \n",
    "    recording=recording,\n",
    "    sparse=False  # Compute on all channels for small tetrode\n",
    ")\n",
    "\n",
    "# Compute all extensions required by the model\n",
    "extensions_to_compute = [\n",
    "    'noise_levels',      # Background noise characterization\n",
    "    'random_spikes',     # Spike sampling for analysis\n",
    "    'waveforms',         # Spike waveform extraction  \n",
    "    'templates',         # Average waveform templates\n",
    "    'spike_locations',   # Spatial localization of spikes\n",
    "    'spike_amplitudes',  # Amplitude distributions\n",
    "    'correlograms',      # Auto/cross-correlograms\n",
    "    'principal_components',  # PCA analysis\n",
    "    'quality_metrics',   # Standard quality metrics\n",
    "    'template_metrics'   # Template-based metrics\n",
    "]\n",
    "\n",
    "sorting_analyzer.compute(extensions_to_compute)\n",
    "print(f\"✓ Computed {len(extensions_to_compute)} analysis extensions\")\n",
    "\n",
    "# Display summary statistics\n",
    "units = sorting_analyzer.unit_ids\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"Unit IDs: {units}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98crgSfL2bfI"
   },
   "source": [
    "### Validating Metric Compatibility\n",
    "\n",
    "Before applying the model, let's verify that our computed metrics match the model's feature requirements. This is a crucial validation step to ensure successful prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1730500724650,
     "user": {
      "displayName": "neuro work",
      "userId": "09036868344714389966"
     },
     "user_tz": -60
    },
    "id": "BfTzH0x92bfI",
    "outputId": "09f3368d-1d7c-4a82-96d6-dba52e79a0ac"
   },
   "outputs": [],
   "source": [
    "# Validate that computed metrics match model requirements\n",
    "quality_metrics = list(sorting_analyzer.get_extension('quality_metrics').get_data().keys())\n",
    "template_metrics = list(sorting_analyzer.get_extension('template_metrics').get_data().keys())\n",
    "all_computed_metrics = quality_metrics + template_metrics\n",
    "\n",
    "print(\"Computed Metrics:\")\n",
    "print(f\"Quality metrics ({len(quality_metrics)}): {quality_metrics[:5]}...\")  # Show first 5\n",
    "print(f\"Template metrics ({len(template_metrics)}): {template_metrics[:5]}...\")\n",
    "\n",
    "print(f\"\\nModel Requirements vs. Computed Metrics:\")\n",
    "required_features = set(model.feature_names_in_)\n",
    "computed_features = set(all_computed_metrics)\n",
    "\n",
    "# Check compatibility\n",
    "missing_features = required_features - computed_features\n",
    "extra_features = computed_features - required_features\n",
    "\n",
    "if len(missing_features) == 0:\n",
    "    print(\"✓ All required metrics are available!\")\n",
    "    compatibility = True\n",
    "else:\n",
    "    print(f\"❌ Missing {len(missing_features)} required metrics: {missing_features}\")\n",
    "    compatibility = False\n",
    "\n",
    "if len(extra_features) > 0:\n",
    "    print(f\"ℹ️  {len(extra_features)} additional metrics computed (not used by model)\")\n",
    "\n",
    "print(f\"\\nCompatibility check: {'PASSED' if compatibility else 'FAILED'}\")\n",
    "print(f\"Feature match: {np.all(sorted(all_computed_metrics) == sorted(model.feature_names_in_))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cyxXn-v2bfI"
   },
   "source": [
    "## Step 4: Applying the Model for Automated Curation\n",
    "\n",
    "### Making Predictions\n",
    "\n",
    "Now we can use our trained model to automatically classify unit quality. The `auto_label_units` function handles feature extraction, preprocessing, and prediction, returning both class labels and confidence scores for each unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 926,
     "status": "ok",
     "timestamp": 1730500733675,
     "user": {
      "displayName": "neuro work",
      "userId": "09036868344714389966"
     },
     "user_tz": -60
    },
    "id": "7N8NWefl2bfI",
    "outputId": "639b7761-ce06-4c92-9356-ed1a58df042e"
   },
   "outputs": [],
   "source": [
    "# Apply model to classify unit quality automatically\n",
    "print(\"Applying pre-trained model to classify units...\")\n",
    "\n",
    "labels = auto_label_units(\n",
    "    sorting_analyzer=sorting_analyzer,\n",
    "    repo_id=\"SpikeInterface/toy_tetrode_model\",\n",
    "    trusted=['numpy.dtype']\n",
    ")\n",
    "\n",
    "print(\"✓ Model predictions completed successfully!\")\n",
    "print(f\"✓ Classified {len(labels)} units\")\n",
    "\n",
    "# Display prediction summary\n",
    "label_counts = labels.iloc[:, 0].value_counts()\n",
    "print(f\"\\nPrediction Summary:\")\n",
    "for label, count in label_counts.items():\n",
    "    percentage = (count / len(labels)) * 100\n",
    "    print(f\"  {label}: {count} units ({percentage:.1f}%)\")\n",
    "        \n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGBcYYH72bfI"
   },
   "source": [
    "### Visualizing Classification Results\n",
    "\n",
    "Let's examine the waveform templates of units with different quality classifications to understand what the model is detecting. We'll compare units with high confidence scores for both 'good' and 'bad' classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "executionInfo": {
     "elapsed": 1300,
     "status": "ok",
     "timestamp": 1730500738100,
     "user": {
      "displayName": "neuro work",
      "userId": "09036868344714389966"
     },
     "user_tz": -60
    },
    "id": "MUAFRtru2bfI",
    "outputId": "e57dbb55-7ea1-4325-b92a-e3598bc34a1b"
   },
   "outputs": [],
   "source": [
    "# Identify representative units for visualization\n",
    "good_units = labels[labels.iloc[:, 0] == 'good']\n",
    "bad_units = labels[labels.iloc[:, 0] == 'bad']\n",
    "\n",
    "# Find highest confidence examples of each class\n",
    "if len(good_units) > 0:\n",
    "    best_good_unit = good_units.iloc[:, 1].idxmax()\n",
    "    good_confidence = good_units.loc[best_good_unit, good_units.columns[1]]\n",
    "    print(f\"Best 'good' unit: {best_good_unit} (confidence: {good_confidence:.1%})\")\n",
    "\n",
    "if len(bad_units) > 0:\n",
    "    best_bad_unit = bad_units.iloc[:, 1].idxmax()\n",
    "    bad_confidence = bad_units.loc[best_bad_unit, bad_units.columns[1]]\n",
    "    print(f\"Best 'bad' unit: {best_bad_unit} (confidence: {bad_confidence:.1%})\")\n",
    "\n",
    "# Plot template comparison\n",
    "units_to_plot = []\n",
    "if len(good_units) > 0:\n",
    "    units_to_plot.append(best_good_unit)\n",
    "if len(bad_units) > 0:\n",
    "    units_to_plot.append(best_bad_unit)\n",
    "\n",
    "if len(units_to_plot) >= 2:\n",
    "    print(f\"\\nComparing templates for units {units_to_plot}\")\n",
    "    sw.plot_unit_templates(sorting_analyzer, unit_ids=units_to_plot)\n",
    "    plt.suptitle(\"Model Classification Comparison: Good vs Bad Units\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Only one class detected - plotting all available units\")\n",
    "    sw.plot_unit_templates(sorting_analyzer, unit_ids=sorting_analyzer.unit_ids[:4])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uVJb2F12bfI"
   },
   "source": [
    "## Step 5: Model Performance Evaluation\n",
    "\n",
    "### Understanding Model Decisions\n",
    "\n",
    "The visualization above shows the model's decision-making process. Typically, 'good' units have:\n",
    "- Clear, consistent spike waveforms\n",
    "- Appropriate amplitude and duration\n",
    "- Low noise characteristics\n",
    "\n",
    "'Bad' units often exhibit:\n",
    "- Inconsistent or noisy waveforms  \n",
    "- Unusual temporal dynamics\n",
    "- Artifacts or multi-unit activity\n",
    "\n",
    "### Comparing with Human Curation\n",
    "\n",
    "To evaluate model performance objectively, we'll compare predictions against simulated human labels and analyze agreement patterns. This helps determine when to trust automated decisions versus requiring manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display current predictions for reference\n",
    "print(\"Current Model Predictions:\")\n",
    "print(\"=\" * 40)\n",
    "for unit_id in labels.index:\n",
    "    pred_label = labels.loc[unit_id, labels.columns[0]]\n",
    "    confidence = labels.loc[unit_id, labels.columns[1]]\n",
    "    print(f\"Unit {unit_id}: {pred_label} ({confidence:.1%} confidence)\")\n",
    "\n",
    "print(f\"\\nTotal units analyzed: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "executionInfo": {
     "elapsed": 652,
     "status": "ok",
     "timestamp": 1730500742403,
     "user": {
      "displayName": "neuro work",
      "userId": "09036868344714389966"
     },
     "user_tz": -60
    },
    "id": "0p2YDE9c2bfI",
    "outputId": "1549663e-e7a7-4897-aaba-91805b78a3cd"
   },
   "outputs": [],
   "source": [
    "# Simulate conservative human curation for comparison\n",
    "# (In practice, load from Phy: sorting_analyzer.sorting.get_property('quality'))\n",
    "human_labels = ['good', 'good', 'good', 'good', 'good', 'bad', 'good', 'bad', 'good', 'good']\n",
    "\n",
    "print(\"Human vs Model Comparison Setup:\")\n",
    "print(\"Human labels (conservative curator):\", human_labels)\n",
    "\n",
    "# Extract model predictions for analysis\n",
    "model_predictions = [labels.iloc[i, 0] for i in range(len(labels))]\n",
    "prediction_confidences = [labels.iloc[i, 1] for i in range(len(labels))]\n",
    "\n",
    "print(\"Model predictions:\", model_predictions)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(human_labels, model_predictions)\n",
    "balanced_accuracy = balanced_accuracy_score(human_labels, model_predictions)\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy:.1%}\")\n",
    "\n",
    "# Visualize confusion matrix with professional styling\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap='Blues')\n",
    "\n",
    "# Add value annotations\n",
    "for (i, j), value in np.ndenumerate(conf_matrix):\n",
    "    plt.annotate(str(value), xy=(j, i), ha='center', va='center', \n",
    "                color=\"white\" if value > conf_matrix.max()/2 else \"black\", \n",
    "                fontsize=16, fontweight='bold')\n",
    "\n",
    "# Formatting\n",
    "label_conversion = model_info['label_conversion']\n",
    "tick_labels = list(label_conversion.values())\n",
    "plt.xlabel('Model Predictions', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Human Labels (Ground Truth)', fontsize=12, fontweight='bold')\n",
    "plt.xticks(ticks=[0, 1], labels=tick_labels)\n",
    "plt.yticks(ticks=[0, 1], labels=tick_labels)\n",
    "plt.title('Confusion Matrix: Human vs Model Curation', fontsize=14, fontweight='bold')\n",
    "plt.suptitle(f'Balanced Accuracy: {balanced_accuracy:.1%}', fontsize=12)\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(label='Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate detailed metrics\n",
    "true_positives = conf_matrix[1, 1]  # Good correctly identified as good\n",
    "false_positives = conf_matrix[0, 1]  # Bad incorrectly identified as good  \n",
    "false_negatives = conf_matrix[1, 0]  # Good incorrectly identified as bad\n",
    "true_negatives = conf_matrix[0, 0]  # Bad correctly identified as bad\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "\n",
    "print(f\"\\nDetailed Performance Metrics:\")\n",
    "print(f\"Precision (good units): {precision:.1%}\")\n",
    "print(f\"Recall (good units): {recall:.1%}\")\n",
    "print(f\"False positive rate: {false_positives}/{len(human_labels)} units ({false_positives/len(human_labels):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV5u_Oyx2bfI"
   },
   "source": [
    "## Step 6: Confidence-Based Workflow Optimization\n",
    "\n",
    "### Understanding Prediction Confidence\n",
    "\n",
    "Model confidence scores provide crucial information for optimizing curation workflows. High-confidence predictions can often be trusted for automated processing, while low-confidence cases should be flagged for manual review.\n",
    "\n",
    "This analysis helps determine:\n",
    "- **Confidence thresholds** for automated acceptance\n",
    "- **Cases requiring manual review** based on uncertainty\n",
    "- **Model reliability patterns** across different unit types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1730500746411,
     "user": {
      "displayName": "neuro work",
      "userId": "09036868344714389966"
     },
     "user_tz": -60
    },
    "id": "PSo1HuP32bfI",
    "outputId": "e1fad50b-cff7-4145-b9fc-32bbfc701146"
   },
   "outputs": [],
   "source": [
    "# Analyze relationship between prediction confidence and accuracy\n",
    "\n",
    "def calculate_confidence_analysis(label_df, confidence_col='confidence', window_size=3):\n",
    "    \"\"\"Calculate moving average of prediction accuracy by confidence level.\"\"\"\n",
    "    \n",
    "    # Create confidence deciles for grouping\n",
    "    label_df[f'{confidence_col}_decile'] = pd.cut(\n",
    "        label_df[confidence_col], \n",
    "        bins=10, \n",
    "        labels=False, \n",
    "        duplicates='drop'\n",
    "    )\n",
    "    \n",
    "    # Calculate agreement by decile\n",
    "    decile_agreement = label_df.groupby(f'{confidence_col}_decile')['agreement'].mean()\n",
    "    \n",
    "    # Sort by confidence for moving average\n",
    "    label_df_sorted = label_df.sort_values(by=confidence_col)\n",
    "    moving_avg_agreement = label_df_sorted['agreement'].rolling(\n",
    "        window=window_size, \n",
    "        center=True\n",
    "    ).mean()\n",
    "    \n",
    "    return label_df_sorted[confidence_col], moving_avg_agreement, decile_agreement\n",
    "\n",
    "# Extract confidence scores and create analysis dataframe\n",
    "confidences = sorting_analyzer.sorting.get_property('classifier_probability')\n",
    "\n",
    "analysis_df = pd.DataFrame({\n",
    "    'human_label': human_labels,\n",
    "    'model_label': model_predictions,\n",
    "    'confidence': confidences\n",
    "}, index=sorting_analyzer.unit_ids)\n",
    "\n",
    "# Calculate agreement between human and model labels\n",
    "analysis_df['agreement'] = (analysis_df['human_label'] == analysis_df['model_label'])\n",
    "\n",
    "print(\"Confidence Analysis Setup:\")\n",
    "print(f\"Mean confidence: {analysis_df['confidence'].mean():.1%}\")\n",
    "print(f\"Confidence range: {analysis_df['confidence'].min():.1%} - {analysis_df['confidence'].max():.1%}\")\n",
    "print(f\"Overall agreement rate: {analysis_df['agreement'].mean():.1%}\")\n",
    "\n",
    "# Perform confidence analysis\n",
    "conf_sorted, moving_avg, decile_stats = calculate_confidence_analysis(analysis_df, window_size=3)\n",
    "\n",
    "# Create comprehensive confidence plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Moving average plot\n",
    "ax1.plot(conf_sorted, moving_avg, 'b-', linewidth=2, label='Moving Average', alpha=0.8)\n",
    "ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Random Chance')\n",
    "ax1.axhline(y=analysis_df['agreement'].mean(), color='green', linestyle='--', \n",
    "           alpha=0.7, label='Overall Agreement')\n",
    "ax1.set_xlabel('Prediction Confidence', fontweight='bold')\n",
    "ax1.set_ylabel('Agreement with Human Labels', fontweight='bold')\n",
    "ax1.set_title('Confidence vs Agreement (Moving Average)', fontweight='bold')\n",
    "ax1.set_ylim(0.3, 1)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Confidence distribution by agreement\n",
    "agreement_true = analysis_df[analysis_df['agreement'] == True]['confidence']\n",
    "agreement_false = analysis_df[analysis_df['agreement'] == False]['confidence']\n",
    "\n",
    "ax2.hist(agreement_true, bins=10, alpha=0.7, label='Correct Predictions', color='green')\n",
    "ax2.hist(agreement_false, bins=10, alpha=0.7, label='Incorrect Predictions', color='red')\n",
    "ax2.set_xlabel('Prediction Confidence', fontweight='bold')\n",
    "ax2.set_ylabel('Number of Units', fontweight='bold')\n",
    "ax2.set_title('Confidence Distribution by Accuracy', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print confidence-based recommendations\n",
    "high_conf_threshold = 0.8\n",
    "low_conf_threshold = 0.6\n",
    "\n",
    "high_conf_units = analysis_df[analysis_df['confidence'] > high_conf_threshold]\n",
    "low_conf_units = analysis_df[analysis_df['confidence'] < low_conf_threshold]\n",
    "\n",
    "print(f\"\\nWorkflow Recommendations:\")\n",
    "print(f\"High confidence (>{high_conf_threshold:.0%}): {len(high_conf_units)} units - suitable for automation\")\n",
    "print(f\"Low confidence (<{low_conf_threshold:.0%}): {len(low_conf_units)} units - require manual review\")\n",
    "print(f\"Medium confidence: {len(analysis_df) - len(high_conf_units) - len(low_conf_units)} units - case-by-case decision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py311-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
